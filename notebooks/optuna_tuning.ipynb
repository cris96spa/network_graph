{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c806ca3b-eed2-49c2-9564-67d2b595bee6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MLflow with Optuna: Hyperparameter Optimization and Tracking\n",
    "\n",
    "Useful links:\n",
    "- [Mlflow Documentation](https://mlflow.org/docs/latest/getting-started/index.html)\n",
    "- [Tutorials and Examples](https://mlflow.org/docs/latest/tutorials-and-examples/index.html)\n",
    "- [This Guide](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html#MLflow-with-Optuna:-Hyperparameter-Optimization-and-Tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80190b-1532-4ca7-9f73-77b36c4d413a",
   "metadata": {},
   "source": [
    " \n",
    "A critical part of building production-grade models is ensuring that a given model’s parameters are selected to create the best inference set possible. \n",
    "However, the sheer number of combinations and their resultant metrics can become overwhelming to track manually. That’s where tools like MLflow and Optuna come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69f449-329e-495a-9a42-8ed2aa4481e9",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In this notebook, you’ll learn how to integrate MLflow with Optuna for hyperparameter optimization. We’ll guide you through the process of:\n",
    "- Setting up your environment with MLflow tracking.\n",
    "- Generating our training and evaluation data sets.\n",
    "- Defining a partial function that fits a machine learning model.\n",
    "- Using Optuna for hyperparameter tuning.\n",
    "- Leveraging child runs within MLflow to keep track of each iteration during the hyperparameter tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c3eed-55da-4cb6-8da9-1a50aa088d66",
   "metadata": {},
   "source": [
    "## Why Optuna?\n",
    "\n",
    "Optuna is an open-source hyperparameter optimization framework in Python. It provides an efficient approach to searching over hyperparameters, incorporating the latest research and techniques. With its integration into MLflow, every trial can be systematically recorded.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e005a9-5765-4a4b-b09e-9f8fa3b00607",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Child Runs in MLflow:\n",
    "\n",
    "One of the core features we will be emphasizing is the concept of ‘child runs’ in MLflow. When performing hyperparameter tuning, each iteration (or trial) in Optuna can be considered a ‘child run’. This allows us to group all the runs under one primary ‘parent run’, ensuring that the MLflow UI remains organized and interpretable. Each child run will track the specific hyperparameters used and the resulting metrics, providing a consolidated view of the entire optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69e7f5-aa70-4405-9709-b2684824827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e441c-064a-4763-af78-b89e24fbc29d",
   "metadata": {},
   "source": [
    "# Configure the tracking server uri\n",
    "\n",
    "Please see the [guide to running notebooks](https://www.mlflow.org/docs/latest/getting-started/running-notebooks/index.html) to running notebooks here for more information on setting the tracking server uri and configuring access to either managed or self-managed MLflow tracking servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe76c5-47a9-4ec9-94f3-bbc726b2311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: review the links mentioned above for guidance on connecting to a managed tracking server, such as the free Databricks Community Edition\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313c24d-d4d4-4003-b622-465973981e53",
   "metadata": {},
   "source": [
    "### Remember! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e7888-7042-493f-9832-85bb75a895b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "In a terminal, you need to run the backend store URI"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba34d65f-5ec3-4ec4-b2e1-7046038b7870",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ". .venv/bin/activate\n",
    "mlflow server --backend-store-uri file:<path-to-mlflow-store>/mlflow-database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527e120-239f-450e-aee6-bf582a2c3740",
   "metadata": {},
   "source": [
    "## Generate synthetic training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2024689-58cf-49e2-a12f-471dbe33e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_apple_sales_data_with_promo_adjustment(\n",
    "    base_demand: int = 1000,\n",
    "    n_rows: int = 5000,\n",
    "    competitor_price_effect: float = -50.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset for predicting apple sales demand with multiple\n",
    "    influencing factors.\n",
    "\n",
    "    This function creates a pandas DataFrame with features relevant to apple sales.\n",
    "    The features include date, average_temperature, rainfall, weekend flag, holiday flag,\n",
    "    promotional flag, price_per_kg, competitor's price, marketing intensity, stock availability,\n",
    "    and the previous day's demand. The target variable, 'demand', is generated based on a\n",
    "    combination of these features with some added noise.\n",
    "\n",
    "    Args:\n",
    "        base_demand (int, optional): Base demand for apples. Defaults to 1000.\n",
    "        n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.\n",
    "        competitor_price_effect (float, optional): Effect of competitor's price being lower\n",
    "                                                   on our sales. Defaults to -50.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with features and target variable for apple sales prediction.\n",
    "\n",
    "    Example:\n",
    "        >>> df = generate_apple_sales_data_with_promo_adjustment(base_demand=1200, n_rows=6000)\n",
    "        >>> df.head()\n",
    "    \"\"\"\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(9999)\n",
    "\n",
    "    # Create date range\n",
    "    dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]\n",
    "    dates.reverse()\n",
    "\n",
    "    # Generate features\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": dates,\n",
    "            \"average_temperature\": np.random.uniform(10, 35, n_rows),\n",
    "            \"rainfall\": np.random.exponential(5, n_rows),\n",
    "            \"weekend\": [(date.weekday() >= 5) * 1 for date in dates],\n",
    "            \"holiday\": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),\n",
    "            \"price_per_kg\": np.random.uniform(0.5, 3, n_rows),\n",
    "            \"month\": [date.month for date in dates],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Introduce inflation over time (years)\n",
    "    df[\"inflation_multiplier\"] = 1 + (df[\"date\"].dt.year - df[\"date\"].dt.year.min()) * 0.03\n",
    "\n",
    "    # Incorporate seasonality due to apple harvests\n",
    "    df[\"harvest_effect\"] = np.sin(2 * np.pi * (df[\"month\"] - 3) / 12) + np.sin(\n",
    "        2 * np.pi * (df[\"month\"] - 9) / 12\n",
    "    )\n",
    "\n",
    "    # Modify the price_per_kg based on harvest effect\n",
    "    df[\"price_per_kg\"] = df[\"price_per_kg\"] - df[\"harvest_effect\"] * 0.5\n",
    "\n",
    "    # Adjust promo periods to coincide with periods lagging peak harvest by 1 month\n",
    "    peak_months = [4, 10]  # months following the peak availability\n",
    "    df[\"promo\"] = np.where(\n",
    "        df[\"month\"].isin(peak_months),\n",
    "        1,\n",
    "        np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),\n",
    "    )\n",
    "\n",
    "    # Generate target variable based on features\n",
    "    base_price_effect = -df[\"price_per_kg\"] * 50\n",
    "    seasonality_effect = df[\"harvest_effect\"] * 50\n",
    "    promo_effect = df[\"promo\"] * 200\n",
    "\n",
    "    df[\"demand\"] = (\n",
    "        base_demand\n",
    "        + base_price_effect\n",
    "        + seasonality_effect\n",
    "        + promo_effect\n",
    "        + df[\"weekend\"] * 300\n",
    "        + np.random.normal(0, 50, n_rows)\n",
    "    ) * df[\n",
    "        \"inflation_multiplier\"\n",
    "    ]  # adding random noise\n",
    "\n",
    "    # Add previous day's demand\n",
    "    df[\"previous_days_demand\"] = df[\"demand\"].shift(1)\n",
    "    df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n",
    "\n",
    "    # Introduce competitor pricing\n",
    "    df[\"competitor_price_per_kg\"] = np.random.uniform(0.5, 3, n_rows)\n",
    "    df[\"competitor_price_effect\"] = (\n",
    "        df[\"competitor_price_per_kg\"] < df[\"price_per_kg\"]\n",
    "    ) * competitor_price_effect\n",
    "\n",
    "    # Stock availability based on past sales price (3 days lag with logarithmic decay)\n",
    "    log_decay = -np.log(df[\"price_per_kg\"].shift(3) + 1) + 2\n",
    "    df[\"stock_available\"] = np.clip(log_decay, 0.7, 1)\n",
    "\n",
    "    # Marketing intensity based on stock availability\n",
    "    # Identify where stock is above threshold\n",
    "    high_stock_indices = df[df[\"stock_available\"] > 0.95].index\n",
    "\n",
    "    # For each high stock day, increase marketing intensity for the next week\n",
    "    for idx in high_stock_indices:\n",
    "        df.loc[idx : min(idx + 7, n_rows - 1), \"marketing_intensity\"] = np.random.uniform(0.7, 1)\n",
    "\n",
    "    # If the marketing_intensity column already has values, this will preserve them;\n",
    "    #  if not, it sets default values\n",
    "    fill_values = pd.Series(np.random.uniform(0, 0.5, n_rows), index=df.index)\n",
    "    df[\"marketing_intensity\"].fillna(fill_values, inplace=True)\n",
    "\n",
    "    # Adjust demand with new factors\n",
    "    df[\"demand\"] = df[\"demand\"] + df[\"competitor_price_effect\"] + df[\"marketing_intensity\"]\n",
    "\n",
    "    # Drop temporary columns\n",
    "    df.drop(\n",
    "        columns=[\n",
    "            \"inflation_multiplier\",\n",
    "            \"harvest_effect\",\n",
    "            \"month\",\n",
    "            \"competitor_price_effect\",\n",
    "            \"stock_available\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a63e5-583b-44d1-b6de-6b8aaa19bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_apple_sales_data_with_promo_adjustment(base_demand=1_000, n_rows=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777274d-46d5-4506-85ba-0d1cc437159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22148914-466c-42b0-9d26-5711d3b6aa4c",
   "metadata": {},
   "source": [
    "## Feature Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059df27-b510-40a4-bc2b-8aa9ac593ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_correlation_with_demand(df, save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the correlation of each variable in the dataframe with the 'demand' column.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame containing the data, including a 'demand' column.\n",
    "    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None (Displays the plot on a Jupyter window)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute correlations between all variables and 'demand'\n",
    "    correlations = df.corr()[\"demand\"].drop(\"demand\").sort_values()\n",
    "\n",
    "    # Generate a color palette from red to green\n",
    "    colors = sns.diverging_palette(10, 130, as_cmap=True)\n",
    "    color_mapped = correlations.map(colors)\n",
    "\n",
    "    # Set Seaborn style\n",
    "    sns.set_style(\n",
    "        \"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5}\n",
    "    )  # Light grey background and thicker grid lines\n",
    "\n",
    "    # Create bar plot\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(correlations.index, correlations.values, color=color_mapped)\n",
    "\n",
    "    # Set labels and title with increased font size\n",
    "    plt.title(\"Correlation with Demand\", fontsize=18)\n",
    "    plt.xlabel(\"Correlation Coefficient\", fontsize=16)\n",
    "    plt.ylabel(\"Variable\", fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.grid(axis=\"x\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # prevent matplotlib from displaying the chart every time we call this function\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Save the plot if save_path is specified\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format=\"png\", dpi=600)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Test the function\n",
    "correlation_plot = plot_correlation_with_demand(df, save_path=\"correlation_plot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32427cd-41bf-4266-a595-8a80575ce4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596c554-c9b3-4896-b395-b99f07145f64",
   "metadata": {},
   "source": [
    "## Residual plot\n",
    "\n",
    "The plot_residuals function serves to visualize the residuals—the differences between the model’s predictions and the actual values in the validation set. Residual plots are crucial diagnostic tools in machine learning, as they can reveal patterns that suggest our model is either failing to capture some aspect of the data or that there’s a systematic issue with the model itself\n",
    "\n",
    "### advantages\n",
    "\n",
    "- Identifying Bias: If residuals show a trend (not centered around zero), it might indicate that your model is systematically over- or under-predicting the target variable.\n",
    "- Heteroskedasticity: Varying spread of residuals across the range of the predicted values can indicate ‘Heteroskedasticity,’ which can violate assumptions in some modeling techniques.\n",
    "- Outliers: Points far away from the zero line can be considered as outliers and might warrant further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdaacd-7702-4b3e-a48d-b1a653939f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(model, dvalid, valid_y, save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the residuals of the model predictions against the true values.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained XGBoost model.\n",
    "    - dvalid (xgb.DMatrix): The validation data in XGBoost DMatrix format.\n",
    "    - valid_y (pd.Series): The true values for the validation set.\n",
    "    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None (Displays the residuals plot on a Jupyter window)\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict using the model\n",
    "    preds = model.predict(dvalid)\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals = valid_y - preds\n",
    "\n",
    "    # Set Seaborn style\n",
    "    sns.set_style(\"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5})\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(valid_y, residuals, color=\"blue\", alpha=0.5)\n",
    "    plt.axhline(y=0, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "    # Set labels, title and other plot properties\n",
    "    plt.title(\"Residuals vs True Values\", fontsize=18)\n",
    "    plt.xlabel(\"True Values\", fontsize=16)\n",
    "    plt.ylabel(\"Residuals\", fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.grid(axis=\"y\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if save_path is specified\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format=\"png\", dpi=600)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6295f-c152-4233-9119-7e2a48820af4",
   "metadata": {},
   "source": [
    "## XGBoost Feature Importance Analysis\n",
    "\n",
    "The plot_feature_importance function is designed to visualize the importance of each feature used in our XGBoost model. Understanding feature importance can offer critical insights into the model’s decision-making process and can aid in feature selection, engineering, and interpretation.\n",
    "\n",
    "\n",
    "### Types of Feature Importance\n",
    "\n",
    "XGBoost offers multiple ways to interpret feature importance. This function supports:\n",
    "\n",
    "- Weight: Number of times a feature appears in a tree across the ensemble of trees (for gblinear booster).\n",
    "\n",
    "- Gain: Average gain (or improvement to the model) of the feature when it is used in trees (for other booster types).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cb039-b69c-4988-9165-96619e8cfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, booster):\n",
    "    \"\"\"\n",
    "    Plots feature importance for an XGBoost model.\n",
    "\n",
    "    Args:\n",
    "    - model: A trained XGBoost model\n",
    "\n",
    "    Returns:\n",
    "    - fig: The matplotlib figure object\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    importance_type = \"weight\" if booster == \"gblinear\" else \"gain\"\n",
    "    xgb.plot_importance(\n",
    "        model,\n",
    "        importance_type=importance_type,\n",
    "        ax=ax,\n",
    "        title=f\"Feature Importance based on {importance_type}\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4060a2d-1a98-48d4-95e9-1cc75091e832",
   "metadata": {},
   "source": [
    "## Setting Up the MLflow Experiment\n",
    "\n",
    "Before we start our hyperparameter tuning process, we need to designate a specific “experiment” within MLflow to track and log our results. An experiment in MLflow is essentially a named set of runs. Each run within an experiment tracks its own parameters, metrics, tags, and artifacts.\n",
    "\n",
    "\n",
    "### Why create a new experiment?\n",
    "\n",
    "- Organization: It helps in keeping our runs organized under a specific task or project, making it easier to compare and analyze results.\n",
    "\n",
    "- Isolation: By isolating different tasks or projects into separate experiments, we prevent accidental overwrites or misinterpretations of results.\n",
    "\n",
    "The get_or_create_experiment function we’ve defined below aids in this process. It checks if an experiment with the specified name already exists. If yes, it retrieves its ID. If not, it creates a new experiment and returns its ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf544e-de9e-45ed-921c-95d7b54bdfca",
   "metadata": {},
   "source": [
    "## experiment_id \n",
    "\n",
    "experiment_id ensures that the run, along with its nested child runs, gets logged under the correct experiment. It provides a structured way to navigate, compare, and analyze our tuning results within the MLflow UI.\n",
    "\n",
    "When we want to try additional parameter ranges, different parameters, or a slightly modified dataset, we can use this Experiment to log all parent runs to keep our MLflow Tracking UI clean and easy to navigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393de45f-2c32-44c3-b31b-d25b79a196ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(experiment_name):\n",
    "    \"\"\"\n",
    "    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n",
    "\n",
    "    This function checks if an experiment with the given name exists within MLflow.\n",
    "    If it does, the function returns its ID. If not, it creates a new experiment\n",
    "    with the provided name and returns its ID.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "    - str: ID of the existing or newly created MLflow experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918f85b-f7cb-41ec-9a17-d178105b4ae8",
   "metadata": {},
   "source": [
    "## Create an experiment for our hyperparameter tuning runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a55866-376b-4e48-84cd-46b69b6907a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = get_or_create_experiment(\"Apples Demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7279d-ee16-4a59-90bf-cef97b883b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current active MLflow experiment\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = df.drop(columns=[\"date\", \"demand\"])\n",
    "y = df[\"demand\"]\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "dvalid = xgb.DMatrix(valid_x, label=valid_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ec02a-f78c-4c76-9564-9e8bbc101755",
   "metadata": {},
   "source": [
    "# Setting Up MLflow and Data Preprocessing for Model Training\n",
    "\n",
    "This section of the code accomplishes two major tasks: initializing an MLflow experiment for usage in run tracking and preparing the dataset for model training and validation.\n",
    "\n",
    "## MLflow Initialization\n",
    "\n",
    "We start by setting the MLflow experiment using the set_experiment function. The experiment_id serves as a unique identifier for the experiment, allowing us to segregate and manage different runs and their associated data efficiently.\n",
    "## Data Preprocessing\n",
    "\n",
    "The next steps involve preparing the dataset for model training:\n",
    "\n",
    "- Feature Selection: We drop the columns ‘date’ and ‘demand’ from our DataFrame, retaining only the feature columns in X.\n",
    "\n",
    "- Target Variable: The ‘demand’ column is designated as our target variable y.\n",
    "\n",
    "- Data Splitting: We split the dataset into training (train_x, train_y) and validation (valid_x, valid_y) sets using a 75-25 split.\n",
    "\n",
    "- XGBoost Data Format: Finally, we convert the training and validation datasets into XGBoost’s DMatrix format. This optimized data structure speeds up the training process and is required for using XGBoost’s advanced functionalities.\n",
    "\n",
    "## Why These Steps Matter\n",
    "\n",
    "- MLflow Tracking: Initializing the MLflow experiment ensures that all subsequent model runs, metrics, and artifacts are logged under the same experiment, making it easier to compare and analyze different models. While we are using the fluent API to do this here, you can also specify the experiment_id within a start_run() context.\n",
    "\n",
    "- Data Preparation: Properly preparing your data ensures that the model training process will proceed without issues and that the results will be as accurate as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3201f-7bea-43d0-865f-f7b4cc890a40",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning and Model Training using Optuna and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4ec90-1248-4963-8fa7-2659b2d7f7ba",
   "metadata": {},
   "source": [
    "The objective function serves as the core of our hyperparameter tuning process using Optuna. Additionally, it trains an XGBoost model using the selected hyperparameters and logs metrics and parameters to MLflow.\n",
    "## MLflow Nested Runs\n",
    "\n",
    "The function starts a new nested run in MLflow. Nested runs are useful for organizing hyperparameter tuning experiments as they allow you to group individual runs under a parent run.\n",
    "## Defining Hyperparameters\n",
    "\n",
    "Optuna’s trial.suggest_* methods are used to define a range of possible values for hyperparameters. Here’s what each hyperparameter does:\n",
    "\n",
    "- objective and eval_metric: Define the loss function and evaluation metric.\n",
    "\n",
    "- booster: Type of boosting to be used (gbtree, gblinear, or dart).\n",
    "\n",
    "- lambda and alpha: Regularization parameters.\n",
    "\n",
    "- Additional parameters like max_depth, eta, and gamma are specific to tree-based models (gbtree and dart).\n",
    "\n",
    "## Model Training\n",
    "\n",
    "An XGBoost model is trained using the chosen hyperparameters and the preprocessed training dataset (dtrain). Predictions are made on the validation set (dvalid), and the mean squared error (mse) is calculated.\n",
    "## Logging with MLflow\n",
    "\n",
    "All the selected hyperparameters and metrics (mse and rmse) are logged to MLflow for later analysis and comparison.\n",
    "\n",
    "- mlflow.log_params: Logs the hyperparameters.\n",
    "\n",
    "- mlflow.log_metric: Logs the metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc5929-6d31-4c08-b530-cb2be5b3bb11",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "1)  Adjust Optuna’s logging level to report only errors, ensuring a decluttered stdout.\n",
    "\n",
    "2)  Define a champion_callback function, tailored to log only when a trial surpasses the previously recorded best metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e11328-13de-4a87-9c59-453b2e1dc84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "\n",
    "    Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "    workers or agents.\n",
    "    The race conditions with file system state management for distributed trials will render\n",
    "    inconsistent values with this callback.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86246b-c87b-42ee-8217-324ca9ee1840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Define hyperparameters\n",
    "        params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        }\n",
    "\n",
    "        if params[\"booster\"] == \"gbtree\" or params[\"booster\"] == \"dart\":\n",
    "            params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "            params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "            params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            params[\"grow_policy\"] = trial.suggest_categorical(\n",
    "                \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "            )\n",
    "\n",
    "        # Train XGBoost model\n",
    "        bst = xgb.train(params, dtrain)\n",
    "        preds = bst.predict(dvalid)\n",
    "        error = mean_squared_error(valid_y, preds)\n",
    "\n",
    "        # Log to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"mse\", error)\n",
    "        mlflow.log_metric(\"rmse\", math.sqrt(error))\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051533f8-9e32-4319-bb5e-f58292a5e7b1",
   "metadata": {},
   "source": [
    "# Orchestrating Hyperparameter Tuning, Model Training, and Logging with MLflow\n",
    "\n",
    "This section of the code serves as the orchestration layer, bringing together Optuna for hyperparameter tuning and MLflow for experiment tracking.\n",
    "Initiating Parent Run\n",
    "\n",
    "We begin by starting a parent MLflow run with the name “Best Run”. All subsequent operations, including Optuna’s trials, are nested under this parent run, providing a structured way to organize our experiments.\n",
    "Hyperparameter Tuning with Optuna\n",
    "\n",
    "- study = optuna.create_study(direction='minimize'): We create an Optuna study object aiming to minimize our objective function.\n",
    "\n",
    "- study.optimize(objective, n_trials=10): The objective function is optimized over 10 trials.\n",
    "\n",
    "## Logging Best Parameters and Metrics\n",
    "\n",
    "After Optuna finds the best hyperparameters, we log these, along with the best mean squared error (mse) and root mean squared error (rmse), to MLflow.\n",
    "## Logging Additional Metadata\n",
    "\n",
    "Using mlflow.set_tags, we log additional metadata like the project name, optimization engine, model family, and feature set version. This helps in better categorizing and understanding the context of the model run.\n",
    "## Model Training and Artifact Logging\n",
    "\n",
    "- We train an XGBoost model using the best hyperparameters.\n",
    "\n",
    "- Various plots—correlation with demand, feature importance, and residuals—are generated and logged as artifacts in MLflow.\n",
    "\n",
    "## Model Serialization and Logging\n",
    "\n",
    "Finally, the trained model is logged to MLflow using mlflow.xgboost.log_model, along with an example input and additional metadata. The model is stored in a specified artifact path and its URI is retrieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f346ecd-64a8-4866-937a-8fb711c4c395",
   "metadata": {},
   "source": [
    "## Naming Runs:\n",
    "\n",
    "- Reference by Name: While MLflow provides unique identifying keys like run_id for each run, having a descriptive name allows for more intuitive referencing, especially when using particular APIs and navigating the MLflow UI.\n",
    "\n",
    "- Clarity and Context: A well-chosen run name can provide context about the hypothesis being tested or the specific modifications made, aiding in understanding the purpose and rationale of a particular run.\n",
    "\n",
    "- Automatic Naming: If you don’t specify a run name, MLflow will generate a unique fun name for you. However, this might lack the context and clarity of a manually chosen name.\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "When naming your runs, consider the following:\n",
    "\n",
    "- Relevance to Code Changes: The name should reflect any code or parameter modifications made for that run.\n",
    "\n",
    "- Iterative Runs: If you’re executing multiple runs iteratively, it’s a good idea to update the run name for each iteration to avoid confusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62ef17-53c6-49c9-baef-8138005d05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"Aperiflow\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339c92c-4584-45da-ba6e-364ecf6dab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the parent run and call the hyperparameter tuning child run logic\n",
    "with mlflow.start_run(experiment_id=experiment_id, \n",
    "                      run_name=run_name, \n",
    "                      nested=True):\n",
    "    # Initialize the Optuna study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    # Execute the hyperparameter optimization trials.\n",
    "    # Note the addition of the `champion_callback` inclusion to control our logging\n",
    "    study.optimize(objective, n_trials=200, callbacks=[champion_callback])\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_mse\", study.best_value)\n",
    "    mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))\n",
    "\n",
    "    # Log tags\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Apple Demand Project\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"xgboost\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log a fit model instance\n",
    "    model = xgb.train(study.best_params, dtrain)\n",
    "\n",
    "    # Log the correlation plot\n",
    "    mlflow.log_figure(figure=correlation_plot, artifact_file=\"correlation_plot.png\")\n",
    "\n",
    "    # Log the feature importances plot\n",
    "    importances = plot_feature_importance(model, booster=study.best_params.get(\"booster\"))\n",
    "    mlflow.log_figure(figure=importances, artifact_file=\"feature_importances.png\")\n",
    "\n",
    "    # Log the residuals plot\n",
    "    residuals = plot_residuals(model, dvalid, valid_y)\n",
    "    mlflow.log_figure(figure=residuals, artifact_file=\"residuals.png\")\n",
    "\n",
    "    artifact_path = \"model\"\n",
    "\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=model,\n",
    "        artifact_path=artifact_path,\n",
    "        input_example=train_x.iloc[[0]],\n",
    "        model_format=\"ubj\",\n",
    "        metadata={\"model_data_version\": 1},\n",
    "    )\n",
    "\n",
    "    # Get the logged model uri so that we can load it from the artifact store\n",
    "    model_uri = mlflow.get_artifact_uri(artifact_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8721b063-2fe4-4127-b705-d82befc9d374",
   "metadata": {},
   "source": [
    "# In Summary\n",
    "\n",
    "The model_uri serves as a consistent, abstracted reference to your model and its associated data. It simplifies interactions with MLflow, ensuring that users don’t need to worry about the specifics of underlying storage mechanisms and can focus on the machine learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2a83e-7a2e-438c-ad5f-d9d3f03949dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5596d-2c64-49d4-9fd6-8e30719daf3f",
   "metadata": {},
   "source": [
    "## Loading the Trained Model with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd929f-5e7e-4bb6-b4d3-8037386672d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = mlflow.xgboost.load_model(model_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001b2b4-5231-4090-adf0-18b4fa07cab1",
   "metadata": {},
   "source": [
    "\n",
    "## Example: Batch Inference Using the Loaded Model\n",
    "\n",
    "After loading the model natively, performing batch inference is straightforward.\n",
    "\n",
    "In the following cell, we’re going to perform a prediction based on the entire source feature set. Although doing an inference action on the entire training and validation dataset features is of very limited utility in a real-world application, we’ll use our generated synthetic data here to illustrate using the native model for inference.\n",
    "Performing Batch Inference and Augmenting Data\n",
    "\n",
    "In this section, we’re taking our entire dataset and performing batch inference using our loaded XGBoost model. We’ll then append these predictions back into our original dataset to compare, analyze, or further process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c182bd-6bbc-4ad3-baf7-cd56f93416ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dmatrix = xgb.DMatrix(X)\n",
    "\n",
    "inference = loaded.predict(batch_dmatrix)\n",
    "\n",
    "infer_df = df.copy()\n",
    "\n",
    "infer_df[\"predicted_demand\"] = inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e00147-f88a-4243-95bf-f4821778640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e409b265-7346-4a36-95e9-fbdc40d6c6fd",
   "metadata": {},
   "source": [
    "# Wrapping Up: Reflecting on Our Comprehensive Machine Learning Workflow\n",
    "\n",
    "Throughout this guide, we embarked on a detailed exploration of an end-to-end machine learning workflow. We began with data preprocessing, delved deeply into hyperparameter tuning with Optuna, leveraged MLflow for structured experiment tracking, and concluded with batch inference.\n",
    "## Key Takeaways\n",
    "\n",
    "- Hyperparameter Tuning with Optuna: We harnessed the power of Optuna to systematically search for the best hyperparameters for our XGBoost model, aiming to optimize its performance.\n",
    "\n",
    "- Structured Experiment Tracking with MLflow: MLflow’s capabilities shone through as we logged experiments, metrics, parameters, and artifacts. We also explored the benefits of nested child runs, allowing us to logically group and structure our experiment iterations.\n",
    "\n",
    "- Model Interpretation: Various plots and metrics equipped us with insights into our model’s behavior. We learned to appreciate its strengths and identify potential areas for refinement.\n",
    "\n",
    "- Batch Inference: The nuances of batch predictions on extensive datasets were explored, alongside methods to seamlessly integrate these predictions back into our primary data.\n",
    "\n",
    "- Logging Visual Artifacts: A significant portion of our journey emphasized the importance of logging visual artifacts, like plots, to MLflow. These visuals serve as invaluable references, capturing the state of the model, its performance, and any alterations to the feature set that might sway the model’s performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd20f6-78d8-4de8-894f-19e1647be676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de0bed-2ef3-4537-ae17-b75ce964d3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
